The surrogate losses from the previous sections can be computed and differentiated with a minor change to a typical policy gradient implementation. For implementations that use automatic differentation, one simply constructs the loss L^CLIP or L^KLPEN instead of LPG, and one performs multiple steps of stochastic gradient ascent on this objective.
Most techniques for computing variance-reduced advantage-function estimators make use of a learned state-value function V^(s_t); for example, generalized advantage estimation, or the finite-horizon estimators. If using a neural network architecture that shares parameters between the policy and value function, we must use a loss function that combines the policy surrogate and a value function error term. This objective can further be augmented by adding an entropy bonus to ensure sufficient exploration, as suggested in past work. Combining these terms, we obtain an objective, which is approximately maximized each iteration: it is the expectation of L^CLIP minus c_1 times L^VF plus c_2 times the entropy bonus. Here, L^VF denotes a squared-error loss measuring the distance of V^s from the target of V^s.
One style of policy gradient implementation, well-suited for use with recurrent neural networks, runs the policy for T timesteps, where T is much less than the episode length, and uses the collected samples for an update. This style requires an advantage estimator that does not look beyond timestep T. The estimator used in past work is A^t = -V^(s_t) + r^t + Gamma * r^(t+1) + Gamma^2 * r^(t+2) + ... + Gamma^(T-t+1) * r^(T-1) + Gamma^(T-t) V^(s_T). Here, "t" specifies the time index in [0, T], within a given length-T trajectory segment. Generalizing this choice, we can use a truncated version of generalized advantage estimation, which reduces to the previous A^t when we set Lambda to 1:
A^t = Delta_t + (Gamma * Lambda) * Delta_(t+1) + ... + (Gamma & Lambda)^(T-t+1) * Delta_(T-1),
where Delta_t = r^t + Gamma * V^(s_(t+1)) - V^(s_t)

A proximal policy optimization (PPO) algorithm that uses fixed-length trajectory segments is shown below. Each iteration, each of N parallel actors collect T timesteps of data. Then we construct the surrogate loss on these N * T timesteps of data, and optimize it with minibatch SGD (or usually for better performance, Adam), for K epochs.